\chapter{Homomorphic Encryption}

The main idea of homomorphic encryption is performing operations on encrypted while still being able to decrypt the data and have the operations remain.

For 30 years after the problem of constructing a fully homomorphic encryption (FHE) scheme was proposed in 1978, there were only partial results. For example, the RSA cryptosystem allows for an unbounded number of modular multiplications, the Goldwater-Micali cryptosystem allows for an unbounded number of exclusive or operations, and the Boheh-Goh-Nissim cryptosystem allows for an unlimited number of addition operations but only one multiplication operation \cite{10.1145/359340.359342}\cite{10.1145/800070.802212}\cite{10.1007/978-3-540-30576-7_18}.

In 2009 Craig Gentry described the first possible construction of a fully homomorphic encryption scheme. Gentry's scheme is a lattice-based scheme which allows for both addition and multiplication operations. With these two operations being possible on the ciphertext, virtually any typical computation can be performed on the original data while it remains encrypted \cite{gentry}.

\section{Lattice-Based Cryptography}

A lattice, typically denoted by $L$, is a subset of $\mathbb{R}^n$ consisting of the integer linear combinations of basis vectors $\mathbf{b}_1, \dots, \mathbf{b}_n \in \mathbb{R}^n$. A specific example of this would be $\mathbb{Z}^2$, the $2$-dimensional integer lattice, which can be generated by the standard basis of $\mathbb{R}^2$. Lattice-based cryptography relies off the hardness of solving the shortest vector problem (SVP), which is known to be NP-hard.

\subsection{Shortest Vector Problem}

The SVP provides a basis $B = \{\mathbf{b}_1, \dots, \mathbf{b}_n\}$ of some vector space $V$ and a norm $N$. To solve the given SVP, one must find the shortest non-zero vector, when measured by $N$, in the lattice $L \subset V$ where $L$ consists of all integer combinations of basis vectors $b_i \in B$. The following provides a simple example which can be visualized:

\begin{example}\label{ex:R2SVP}
    Let 
    \[
        B := \left\{\mathbf{b}_1 = [2, 1]^t, \mathbf{b}_2 = [4, 1]^t\right\},
    \]
    $N$ be the Euclidean norm, and $L \subset \mathbb{R}^2$ be the set of all integer combinations of vectors in $B$. We want to find the shortest non-zero vector in $L$. The vectors $2\mathbf{b}_1 - \mathbf{b}_2 = [0, 1]^t$ and $\mathbf{b}_2 - \mathbf{b}_1 = [2, 0]^t$ provide us with a different basis that give an easy visualization of the shortest vector in $L$, which can be written as $[0, 1]^t$.
\end{example}

Unlike Example \ref{ex:R2SVP}, when taking advantage of the SVP in implementation we are looking at working in linear spaces with thousands of dimensions. 

When working in these lattices we can have what are referred to as good and bad basis. In essence, shorter basis vectors that can be used to build a lattice are considered to be a ``good" basis and longer basis vectors that can be used to build the same lattice are considered a ``bad" basis. \color{red} (might need to add more stuff here) \color{black}

A generalization of the SVP exists known as the closest vector problem \cite{micciancio2012complexity}.

\subsection{Closest Vector Problem}

The CVP provides a basis $B$ of some vector space $V$ and a metric $M$. Once again we have the lattice $L$ which consists of of all integer combination of the basis vectors in $B$. To solve the CVP, one is provided with some vector $\mathbf{v} \in V$ and must find the closest vector to $\mathbf{v}$ in $L$ when measured using $M$. The following provides another simple example which can be visualized:

\begin{example}\label{ex:R2CVP}
    Let
    \[
        B := \left\{\mathbf{b}_1 = [2, 1]^t, \mathbf{b}_2 = [4, 1]^t\right\},
    \]
    $M$ be the Euclidean metric, and $L \subset \mathbb{R}^2$ be the set of all integer combinations of vectors in $B$. If provided with $\mathbf{v} \in V$ where $\mathbf{v} = [-0.03, 0.97]^t$ one can intuitively see that the vector $2\mathbf{b}_1 - \mathbf{b}_2 = [0, 1]^t$ is the closest vector to $\mathbf{v}$ in $L$.
\end{example}

\color{red} (might need to add more stuff here) \color{black}

\section{Gentry's Bootstrapping Method}

We will express the general idea of Gentry's bootstrapping method of FHE. Suppose that we have a set of data $\{D_1, \dots, D_n\}$ and an encryption scheme, which we will denote as a function $f$, in which some noise is added to the data during the encryption process. Suppose the chosen encryption scheme is capable of correcting up to a maximum of $t$ errors worth of noise. Typically, as seen in subsection \ref{subsec:mceliecealgo}, we might add $t$ errors, the maximum amount, during the encryption process of such a scheme. If we add less than the maximum amount of noise during the encryption process the scheme is still capable of decrypting the ciphertext. There exists encryption schemes, such as lattice or code based schemes, which are naturally at least partially homomorphic in that performing operations between ciphertext compound noise until the scheme will no longer perform a correct decryption (since the noise will eventually exceed $t$ total errors). For this example we will assume we have left ourselves enough error correction capability, after the encryption process, to perform several operations before exceeding $t$ total errors. 

Suppose we wish to perform some arbitrary amount of operations on our set of ciphertext $\{f(D_1), \dots, f(D_n)\}$. We first perform the maximum amount of operations we are capable of before exceeding our noise threshold $t$,
\begin{equation}\label{homomoprhic}
    f(D_i) + \cdots + f(D_j) * \cdots * f(D_k) = f(D_i + \cdots + D_j * \cdots * D_k).
\end{equation}
Notice, from equation \eqref{homomoprhic}, that we are already working in a partially homomorphic scheme as described above. At this point, performing any additional operations will cause a decryption error since we will have exceeded $t$ total noise. In order to proceed with Gentry's bootstrapping method we need to be working with a scheme that also allows three-pass protocol. Such schemes allow for a second encryption while still being able to decrypt the original decryption. 

The idea here is that we need a way to ``reset" the accumulating noise so that we can essentially start over. If we encrypt our ciphertext using a second encryption, which we can denote as the function $g$, we can then safely decrypt our original encryption (as done in a three-pass protocol) without compromising our data.

